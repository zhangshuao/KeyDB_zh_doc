# 深入集群教程

本文档文静地介绍了KeyDB集群，它不使用complex来理解分布式系统的概念。
它提供了关于如何设置集群、测试和操作它的说明，而没有涉及KeyDB集群规范中涉及的细节，只是从用户的角度描述了系统的行为。

然而，本教程试图从最终用户的角度提供关于KeyDB集群的可用性和一致性特征的信息，以一种简单易懂的方式进行说明。

注意本教程需要KeyDB 3.0或更高版本。

如果您计划运行严格的KeyDB集群部署，建议阅读更正式的规范，即使不是严格要求的。
但是，最好先从这个文档开始，过一段时间再使用KeyDB集群，然后再阅读规范。

## KeyDB集群101

KeyDB集群提供了一种运行KeyDB安装的方法，其中**数据被跨多个KeyDB节点自动分片。**

KeyDB集群还在分区期间提供了一定程度的可用性，这实际上是在某些节点失败或无法通信时继续操作的能力。
但是，在发生较大的故障时(例如，当大多数主机不可用时)，集群将停止运行。

实际上，KeyDB集群有什么用呢?

* **在多个节点之间自动分割数据集**的能力。

* **当节点子集发生故障或无法与集群的其他部分通信时，继续操作**的能力。

## KeyDB集群TCP端口

每个KeyDB集群节点都需要打开两个TCP连接。用于服务客户端的普通KeyDB TCP端口，例如6379，加上通过向数据端口添加10000获得的端口，即示例中的16379。

第二个高端口用于集群总线，它是一个使用二进制协议的节点到节点通信通道。节点使用集群总线进行故障检测、配置更新、故障转移授权等。
客户端不应该尝试与集群总线端口通信，而应该始终使用正常的KeyDB命令端口，但是要确保打开防火墙中的两个端口，否则KeyDB集群节点将无法通信。

命令端口和集群总线端口偏移是固定的，总是10000。

注意，要使KeyDB集群正常工作，您需要为每个节点:

1.用于与客户端通信的普通客户端通信端口(通常为6379)对需要到达集群的所有客户端开放，
加上所有其他集群节点(使用客户端端口进行key迁移)。
2.集群总线端口(客户端端口 + 10000)必须可以从所有其他集群节点到达。

如果不同时打开两个TCP端口，集群将无法正常工作。

集群总线使用不同的二进制协议进行节点到节点的数据交换，这种协议更适合在节点之间交换信息，只需要很少的带宽和处理时间。

## KeyDB集群和Docker

目前，KeyDB集群不支持受限制的环境，并且在IP地址或TCP端口被重新映射的一般环境中也不支持。

Docker使用了一种称为端口映射的技术: 在Docker容器中运行的程序可能与它认为正在使用的端口不同。
这对于在同一台服务器上同时运行多个使用相同端口的容器非常有用。

为了使Docker与KeyDB集群兼容，需要使用Docker的主机联网模式。有关更多信息，请查看Docker文档中的--net=host选项。

## KeyDB集群数据分片

KeyDB集群不使用一致的哈希，而是使用一种不同的切分形式，其中每个键在概念上都是我们称为哈希槽的一部分。

KeyDB集群中有16384个哈希槽，要计算给定键的哈希槽，只需取键模的CRC16即可

KeyDB集群中的每个节点负责哈希槽的一个子集，例如，您可能有一个具有3个节点的集群，其中:

    * 节点A包含从0到5500的哈希槽。
    * 节点B包含从5501到11000的哈希槽。
    * 节点C包含从11001到16383的哈希槽。

这允许轻松地添加和删除集群中的节点。例如,如果我想要添加一个新节点D,我需要把一些散列槽从节点,B, C, D .同样的如果我想删除节点的集群可以提供服务的散列槽移动到B和C,当节点是空的我完全可以将它从集群。

因为将哈希槽从一个节点移动到另一个节点不需要停止操作，所以添加和删除节点或更改节点持有的哈希槽的百分比不需要任何停机时间。

只要一个命令执行(或整个事务，或Lua脚本执行)中涉及的所有键都属于同一个散列槽，KeyDB集群就支持多个键操作。
用户可以使用一个称为散列标签的概念强制多个键成为同一个散列槽的一部分。

KeyDB集群中的散列标签记录规范,但主旨是,如果有一个括号{}之间的子串key,只在字符串是散列,例如这{foo}键和另一个{foo}的关键是保证在同一散列槽,并可以在一个命令一起使用多个键作为参数。

## KeyDB集群主从模型

为了在主节点子集出现故障或无法与大多数节点通信时保持可用性，KeyDB集群使用一个主从模型，其中每个散列槽有1个(主节点本身)到N个副本(N-1个额外的从节点)。

在我们的示例集群节点A、B、C中，如果节点B失败，则集群将无法继续，因为我们不再能够提供范围为5501-11000的散列槽。

然而创建集群时(或在稍后的时间)我们slave节点添加到每一个master,所以最终集群由一个B, C是节点,master和A1, B1, C1,slave节点,系统能够继续如果节点B失败。

节点B1复制B, B失败，集群将提升节点B1为新master节点，并继续正常运行。

但是请注意，如果节点B和B1同时失败，KeyDB集群将无法继续操作。

## KeyDB集群一致性保证

KeyDB集群不能保证**强一致性**。实际上，这意味着在某些情况下，KeyDB集群可能会丢失系统向客户端确认的写操作。

KeyDB集群会丢失写操作的第一个原因是它使用异步复制。这意味着在写期间会发生以下情况:

* 您的客户端向master B写入数据。
* master B对你的客户回复OK。
* master B将写操作传播到slave B、B和B3。

可以看到B不等待一个承认从B1, B2, B3回复给客户端之前,因为这将是一个禁止KeyDB延迟罚款,
如果你的客户写东西,B承认写,但崩溃之前能够把写的slave,一个slave(不接受写)可以提升主人,永远失去了写。

这与配置为每秒将数据刷新到磁盘的大多数数据库所发生的情况非常相似，
因此，由于过去使用传统数据库系统时不涉及分布式系统，您已经能够推断出这种情况。
类似地，您可以通过强制数据库在响应客户端之前刷新磁盘上的数据来提高一致性，但这通常会导致性能低下。
这相当于KeyDB集群中的同步复制。

基本上，在性能和一致性之间需要权衡。

KeyDB集群支持同步写道当绝对需要,实现通过等待命令,这使得失去写了很多不太可能,
但是注意,KeyDB集群没有实现强一致性,即使使用同步复制:总是有可能失败在更复杂的场景,一个slave,不能够接收写当选为主。

还有另一个值得注意的场景，其中KeyDB集群将丢失写操作，这发生在一个网络分区中，其中客户机与少数实例(至少包括一个主实例)隔离。

以A、B、C、A1、B1、C1组成的6个节点集群为例，有3个主节点和3个从节点。还有一个客户机，我们将其称为Z1。

一个划分发生后，可能在划分的一边有a C A1 B1 C1，另一边有B和Z1。

Z1仍然可以写给B，它将接受它的写。如果分区在很短的时间内恢复，集群将正常继续。
但是，如果该分区持续足够长的时间，使B1在该分区的主要部分升级为主分区，则Z1发送给B的写操作将丢失。

请注意，对于Z1能够发送给B的写操作数量，有一个最大的窗口:如果已经经过了足够长的时间让分区的主要部分选择一个从节点作为主节点，
那么少数部分的每个主节点都将停止接受写操作。

这个时间量是KeyDB集群的一个非常重要的配置指令，称为节点超时。

当节点超时结束后，一个主节点被认为是失败的，可以用它的一个副本替换。类似地，在节点超时结束后，
没有一个主节点能够感知其他大多数主节点，它进入错误状态，停止接受写操作。

# KeyDB集群配置参数

我们将创建一个示例集群部署。在继续之前，让我们介绍KeyDB集群在keydb.conf文件中引入的配置参数。
有些是显而易见的，有些则会随着你的阅读而变得更加清晰。

* cluster-enabled <yes/no> : 如果yes开启，则在特定的KeyDB实例中启用KeyDB集群支持。否则，该实例将像往常一样作为独立实例启动。
* cluster-config-file <filename> : 注意，尽管该选项名为KeyDB，但它不是一个用户可编辑的配置文件，而是一个KeyDB集群节点在每次发生更改时自动保持集群配置(基本上是状态)的文件，
                                   以便能够在启动时重新读取它。该文件列出了集群中的其他节点、它们的状态、持久变量等等。由于接收到一些消息，这个文件常常被重写并刷新到磁盘上。
* cluster-node-timeout <milliseconds> : KeyDB集群节点不可用的最长时间，而不会被认为是故障。如果一个主节点的访问时间超过了指定的时间量，则它的从节点将进行故障转移。
                                        该参数控制KeyDB集群中的其他重要内容。值得注意的是，在指定的时间内不能到达大多数主节点的节点将停止接受查询。
* cluster-slave-validity-factor <factor> : 如果设置为0，则从服务器将始终尝试对主服务器进行故障转移，而不管主服务器和从服务器之间的连接断开的时间长短。
                                          如果值是正数,最大断开时间计算的节点超时值乘以系数提供了这个选项,如果节点是一个slave,它不会试图启动一个故障转移如果主链接断开连接超过指定的时间。
                                          例如，如果将节点超时设置为5秒，并且有效性因子设置为10，则断开与主节点连接超过50秒的从节点将不会尝试对其主节点进行故障转移。
                                          注意，任何不同于零的值都可能导致KeyDB集群在主故障之后不可用，如果没有从服务器能够对其进行故障转移。在这种情况下，只有当原来的主服务器重新加入集群时，集群才会返回可用状态。
* cluster-migration-barrier <count> :  一个master将保持连接的slave的最小数量，以便另一个slave迁移到一个不再被任何slave覆盖的master。有关更多信息，请参阅本教程中有关复制迁移的适当部分。
* cluster-require-full-coverage <yes/no> : 如果将其设置为yes(这是默认设置)，那么如果某个节点没有覆盖key空间的某个百分比，集群将停止接受写操作。
                                           如果将该选项设置为no，即使只处理关于键子集的请求，集群也将提供查询。

# 创建和使用KeyDB集群

注意:要手动部署KeyDB集群，了解它的某些操作方面非常重要。但是，如果您希望尽快启动并运行一个集群，请跳过本节和下一节，
直接使用create-cluster脚本创建一个KeyDB集群。

要创建集群，首先需要在集群模式下运行几个空KeyDB实例。这基本上意味着不使用普通的KeyDB实例创建集群，
而是需要配置一个特殊模式，以便KeyDB实例能够启用集群的特定功能和命令。

下面是一个最小的KeyDB集群配置文件:

    port 7000
    cluster-enabled yes
    cluster-config-file nodes.conf
    cluster-node-timeout 5000
    appendonly yes

正如您所看到的，启用集群模式的只是启用集群的指令。每个实例还包含存储此节点配置的文件的路径，默认情况下为nodes.conf。
这个文件从来没有人碰过; 它只是在启动时由KeyDB集群实例生成，并在每次需要时更新。

请注意，按预期工作的最小集群需要包含至少三个主节点。对于您的第一次测试，强烈建议启动一个包含三个主节点和三个从节点的六个节点集群。

为此，输入一个新目录，并创建以下目录，这些目录以我们将在任何给定目录中运行的实例的端口号命名。

某事 例如:

    mkdir cluster-test
    cd cluster-test
    mkdir 7000 7001 7002 7003 7004 7005

在每个目录中(从7000到7005)创建一个keydb.conf文件。作为配置文件的模板，只需使用上面的小示例，但请确保根据目录名用正确的端口号替换端口号7000。

现在，将从GitHub上的不稳定分支的最新源代码编译的keydb-server可执行文件复制到集群测试目录中，最后在您喜欢的终端应用程序中打开6个终端选项卡。

像这样启动每个实例，每个标签:

    cd 7000
    ../keydb-server ./keydb.conf

从每个实例的日志中可以看到，由于不存在nodes.conf文件，每个节点都为自己分配一个新ID。

    [82462] 26 Nov 11:56:55.329 * No cluster configuration found, I'm 97a3a64667477371c4479320d683e4c8db5858b1

此ID将被此特定实例永久使用，以便该实例在集群上下文中拥有唯一的名称。每个节点使用这个id来记住其他节点，而不是通过IP或端口。
IP地址和端口可能会改变，但是唯一的节点标识符在节点的整个生命周期中都不会改变。我们简单地将这个标识符称为**节点ID**

## 创建集群

现在我们已经运行了许多实例，我们需要通过向节点写入一些有意义的配置来创建集群。

如果您正在使用KeyDB 5，这是非常容易实现的，因为我们可以通过嵌入到keydb-cli中的KeyDB集群命令行实用程序获得帮助，
它可以用于创建新的集群、检查或重新切分现有的集群，等等。

对于KeyDB版本3或4，有一个较旧的工具，称为redis-trib.rb也很相似。您可以在KeyDB源代码发行版的src目录中找到它。

第一个示例，即集群的创建，将在KeyDB 5中使用key-cli，在KeyDB 3和KeyDB 4中使用redis-trib。
但是，接下来的所有示例都将只使用keydb-cli，因为您可以看到语法非常相似，并且可以通过使用redis-trib将一个命令行转换成另一个命令行。
rb帮助获取关于旧语法的信息。重要提示:注意，如果您愿意，可以对KeyDB 4集群使用KeyDB 5 keydb-cli，而不会出现任何问题。

要用keydb-cli创建KeyDB 5的集群，只需输入以下命令:
 
    keydb-cli --cluster create 127.0.0.1:7000 127.0.0.1:7001 \
    127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 \
    --cluster-replicas 1

使用KeyDB 4或3类型的redis-trib.rb :

    ./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \
    127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005

这里使用的命令是create，因为我们想要创建一个新的集群。选项--cluster-replicas 1意味着我们希望为每个创建的主机创建一个slave。
其他参数是用于创建新集群的实例的地址列表。

显然，我们的需求的惟一设置是创建一个包含3个主服务器和3个从服务器的集群。

keydb-cli将为您提供一个配置。输入yes接受建议的配置。集群将被配置和连接，这意味着实例将被引导到相互通信。
最后，如果一切顺利，您将看到这样的消息:

    [OK] All 16384 slots covered

这意味着至少有一个主实例为可用的16384个插槽提供服务.

## 使用create-cluster脚本创建KeyDB集群
 
如果您不希望通过如上所述手动配置和执行各个实例来创建KeyDB集群，那么有一个更简单的系统(但是您不会学到相同数量的操作细节)。

只需检查KeyDB发行版中的utils/create-cluster目录。其中有一个名为create-cluster的脚本(与包含它的目录同名)，
它是一个简单的bash脚本。为了启动一个有3个主节点和3个从节点的6个节点集群，只需输入以下命令:
 
1.create-cluster start
2.create-cluster create
 
当keydb-cli实用程序希望您接受集群布局时，在步骤2中回答yes。
您现在可以与集群交互，第一个节点将默认从端口30001开始。完成后，使用以下命令停止集群:

3.create-cluster stop

有关如何运行脚本的更多信息，请阅读此目录中的README。

测试KeyDB集群的一种简单方法是尝试上面的任何客户机，或者使用keydb-cli命令行实用工具。下面是使用后者的交互示例:

    $ keydb-cli -c -p 7000
    KeyDB 127.0.0.1:7000> set foo bar
    -> Redirected to slot [12182] located at 127.0.0.1:7002
    OK
    KeyDB 127.0.0.1:7002> set hello world
    -> Redirected to slot [866] located at 127.0.0.1:7000
    OK
    KeyDB 127.0.0.1:7000> get foo
    -> Redirected to slot [12182] located at 127.0.0.1:7002
    "bar"
    KeyDB 127.0.0.1:7000> get hello
    -> Redirected to slot [866] located at 127.0.0.1:7000
    "world"

**注意:** 如果您使用脚本创建集群，您的节点可能会侦听不同的端口，缺省情况下从30001开始。

keydb-cli集群支持是非常基本的，因此它总是使用KeyDB集群节点能够将客户机重定向到正确的节点这一事实。
严肃的客户端可以做得更好，缓存哈希槽和节点地址之间的映射，直接使用到正确节点的正确连接。
只有在集群配置中发生更改时，例如故障转移后或系统管理员通过添加或删除节点更改集群布局后，才会刷新映射。

## 使用keydb-rb-cluster编写示例应用程序

在继续展示如何操作KeyDB集群、执行故障转移或重新分片等操作之前，我们需要创建一些示例应用程序，或者至少能够理解简单KeyDB集群客户机交互的语义。

通过这种方式，我们可以运行一个示例，同时尝试使节点失败，或者启动重新分片，以查看KeyDB集群在实际情况下的行为。
当没有人向集群写入数据时，查看发生了什么并不是很有帮助。

本节解释keydb-rb-cluster的一些基本用法，并给出两个示例。第一个是 keydb-rb-cluster内部中的example.rb 文件分布:

       1  require './cluster'
       2
       3  if ARGV.length != 2
       4      startup_nodes = [
       5          {:host => "127.0.0.1", :port => 7000},
       6          {:host => "127.0.0.1", :port => 7001}
       7      ]
       8  else
       9      startup_nodes = [
      10          {:host => ARGV[0], :port => ARGV[1].to_i}
      11      ]
      12  end
      13
      14  rc = KeyDBCluster.new(startup_nodes,32,:timeout => 0.1)
      15
      16  last = false
      17
      18  while not last
      19      begin
      20          last = rc.get("__last__")
      21          last = 0 if !last
      22      rescue => e
      23          puts "error #{e.to_s}"
      24          sleep 1
      25      end
      26  end
      27
      28  ((last.to_i+1)..1000000000).each{|x|
      29      begin
      30          rc.set("foo#{x}",x)
      31          puts rc.get("foo#{x}")
      32          rc.set("__last__",x)
      33      rescue => e
      34          puts "error #{e.to_s}"
      35      end
      36      sleep 0.1
      37  }
      
应用程序做了一件非常简单的事情，它将表单foo中的键设置为number，一个接一个。
因此，如果你运行程序的结果是以下命令流:

    SET foo0 0
    SET foo1 1
    SET foo2 2
    等等……

这个程序看起来比通常要复杂得多，因为它被设计成在屏幕上显示错误，而不是带着异常退出，
所以使用集群执行的每个操作都被begin rescue块包装起来。

第14行是程序中第一个有趣的行。它创建KeyDB集群对象，使用启动节点列表作为参数，该对象对不同节点允许的最大连接数，
最后认为给定操作失败后的超时。

启动节点不需要是集群的所有节点。重要的是至少有一个节点是可到达的。还要注意，只要keydb-rb-cluster能够连接到第一个节点，
它就会更新这个启动节点列表。你应该预料到这样的行为对任何其他认真的客户。

现在我们已经将KeyDB集群对象实例存储在rc变量中，我们可以像使用普通KeyDB对象实例那样使用该对象了。

这正是第18行到第26行所发生的:当我们重新启动示例时，我们不希望再次使用foo0启动，因此我们将计数器存储在KeyDB本身中。
上面的代码被设计用来读取这个计数器，或者如果计数器不存在，则将其赋值为0。

但是，请注意这是一个while循环，因为我们希望一次又一次地尝试，即使集群停机并返回错误。
普通的应用程序不需要这么小心。

28和37之间的行启动主循环，在其中设置键或显示错误。

注意循环末尾的sleep调用。在您的测试中，如果您想要尽快地向集群写入数据，您可以删除休眠(相对于这样一个事实，即这是一个繁忙的循环，
当然没有真正的并行性，因此在最好的条件下，您通常可以获得10k ops/秒)。

通常，为了使示例应用程序更容易被人理解，写操作会减慢。

启动应用程序产生以下输出:

    ruby ./example.rb
    1
    2
    3
    4
    5
    6
    7
    8
    9
    ^C (I stopped the program here)
      
这不是一个非常有趣的程序，我们稍后会使用一个更好的程序，但是我们已经可以看到当程序运行时重新分片时会发生什么。
      
## Resharding 集群

现在我们准备尝试集群重新分片。要做到这一点，请保留example.rb程序运行，以便您可以看到是否对程序运行有一些影响。
您可能还希望注释sleep调用，以便在重新分片期间有更严重的写负载。

Resharding基本上意味着将哈希槽从一组节点移动到另一组节点，就像集群创建一样，它是使用keydb-cli实用程序完成的。

要开始重新分片，只需输入:

    keydb-cli --cluster reshard 127.0.0.1:7000

您只需要指定一个节点，keydb-cli将自动找到其他节点。

目前，keydb-cli只能在管理员的支持下重新分片，您不能只说将5%的插槽从这个节点移动到另一个节点(但是实现起来非常简单)。
首先是提问。第一个问题是你想做多大的重新分片:

    How many slots do you want to move (from 1 to 16384)?

我们可以尝试重新切分1000个哈希槽，如果示例仍然在没有sleep调用的情况下运行，那么这些哈希槽应该已经包含了大量的键。

然后keydb-cli需要知道重新分片的目标是什么，即接收散列槽的节点。我将使用第一个主节点，即127.0.0.1:7000，但是我需要指定实例的节点ID。
这已经由keydb-cli打印在一个列表中，但如果需要，我总是可以使用以下命令找到节点的ID:

    $ keydb-cli -p 7000 cluster nodes | grep myself
    97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5460

我的目标节点是97a3a64667477371c4479320d683e4c8db5858b1。

现在你会被问到你想从哪个节点获取这些键。我将键入all，以便从所有其他主节点获取一些散列槽。

在最后确认之后，您将看到keydb-cli将从一个节点移动到另一个节点的每个插槽的消息，并且将为从一边移动到另一边的每个实际密钥打印一个点。

在重新分片的过程中，您应该能够看到示例程序不受影响地运行。如果需要，可以在重新分片期间多次停止和重新启动它。

在重新分片结束时，您可以使用以下命令测试集群的健康状况:

keydb-cli --cluster check 127.0.0.1:7000

所有的槽都将像往常一样覆盖，但是这次主机127.0.0.1:7000将有更多的哈希槽，大约在6461左右。

## 编写重新分片操作的脚本

可以自动执行重新分片，而不需要以交互方式手动输入参数。这是可能使用命令行，如以下:

    keydb-cli reshard <host>:<port> --cluster-from <node-id> --cluster-to <node-id> --cluster-slots <number of slots> --cluster-yes

如果您经常需要重新分片，这允许您构建一些自动化功能，但是目前keydb-cli还没有办法自动地重新平衡集群，检查跨集群节点的key分布，并根据需要智能地移动插槽。
该功能将在未来添加。

## 一个更有趣的示例应用程序

我们早期编写的示例应用程序不是很好。它以一种简单的方式写入集群，甚至不检查写入的内容是否正确。

从我们的观点来看，接收写操作的集群可以总是将键foo写到42来执行每个操作，而我们根本不会注意到。

因此，在keydb-rb集群存储库中，有一个更有趣的应用程序，称为consistency-test.rb。
默认情况下，它使用一组计数器，并发送INCR命令以增加计数器。

然而，除了写之外，应用程序还做了另外两件事:

* 当使用INCR更新计数器时，应用程序将记住写入。
* 它还在每次写之前读取一个随机计数器，并检查该值是否为我们所期望的值，并将其与内存中的值进行比较。

这意味着这个应用程序是一个简单的一致性检查器，能够告诉您集群是否丢失了一些写，或者是否接受了一个我们没有收到确认的写。
在第一种情况下，我们会看到一个计数器的值比我们记忆中的值要小，而在第二种情况下，这个值会更大。

运行一致性测试应用程序每秒钟产生一行输出:

    $ ruby consistency-test.rb
    925 R (0 err) | 925 W (0 err) |
    5030 R (0 err) | 5030 W (0 err) |
    9261 R (0 err) | 9261 W (0 err) |
    13517 R (0 err) | 13517 W (0 err) |
    17780 R (0 err) | 17780 W (0 err) |
    22025 R (0 err) | 22025 W (0 err) |
    25818 R (0 err) | 25818 W (0 err) |

这一行显示了执行的读和写的数量，以及错误的数量(因为系统不可用，所以查询不被接受)。

如果发现不一致，就向输出添加新行。这是所发生的，例如，如果我重置一个计数器手动而程序正在运行:

    $ keydb-cli -h 127.0.0.1 -p 7000 set key_217 0
    OK
    
    (in the other tab I see...)
    
    94774 R (0 err) | 94774 W (0 err) |
    98821 R (0 err) | 98821 W (0 err) |
    102886 R (0 err) | 102886 W (0 err) | 114 lost |
    107046 R (0 err) | 107046 W (0 err) | 114 lost |

当我将计数器设置为0时，实际值是114，因此程序报告114写操作丢失(包括集群无法记住的INCR命令)。

作为测试用例，这个程序要有趣得多，因此我们将使用它来测试KeyDB集群故障转移。

## 测试 故障转移

注意:在此测试期间，您应该打开一个选项卡，以运行一致性测试应用程序。

为了触发故障转移，我们可以做的最简单的事情(这也是分布式系统中可能发生的语义上最简单的故障)是使单个进程崩溃，在我们的示例中是单个主进程。

我们可以识别集群并使用以下命令崩溃它:

    $ keydb-cli -p 7000 cluster nodes | grep master
    3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385482984082 0 connected 5960-10921
    2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 master - 0 1385482983582 0 connected 11423-16383
    97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5959 10922-11422
    
好的，7000 7001 7002是master。让我们使用DEBUG SEGFAULT命令崩溃节点7002:

    $ keydb-cli -p 7002 debug segfault
    Error: Server closed the connection

现在我们可以查看一致性测试的输出，看看它报告了什么。

    18849 R (0 err) | 18849 W (0 err) |
    23151 R (0 err) | 23151 W (0 err) |
    27302 R (0 err) | 27302 W (0 err) |
    
    ... many error warnings here ...
    
    29659 R (578 err) | 29660 W (577 err) |
    33749 R (578 err) | 33750 W (577 err) |
    37918 R (578 err) | 37919 W (577 err) |
    42077 R (578 err) | 42078 W (577 err) |

可以看到，在故障转移期间，系统无法接受578次读和577次写，但是数据库中没有创建不一致。
这听起来可能有些出乎意料，因为在本教程的第一部分中，我们曾提到KeyDB集群在故障转移期间会丢失写操作，因为它使用异步复制。
我们没有说的是，这种情况不太可能发生，因为KeyDB几乎同时向客户机发送应答，并将命令复制到从服务器，因此会有一个非常小的丢失数据的窗口。
然而，很难触发的事实并不意味着不可能触发，因此这不会改变KeyDB集群提供的一致性保证。

我们现在可以检查什么是集群设置后的故障转移(注意，在此期间，我重新启动崩溃的实例，使它重新作为一个slave集群):

    $ keydb-cli -p 7000 cluster nodes
    3fc783611028b1707fd65345e763befb36454d73 127.0.0.1:7004 slave 3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 0 1385503418521 0 connected
    a211e242fc6b22a9427fed61285e85892fa04e08 127.0.0.1:7003 slave 97a3a64667477371c4479320d683e4c8db5858b1 0 1385503419023 0 connected
    97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5959 10922-11422
    3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7005 master - 0 1385503419023 3 connected 11423-16383
    3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385503417005 0 connected 5960-10921
    2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385503418016 3 connected

现在master运行在7000、7001和7005端口。以前的主机是什么，即运行在端口7002上的KeyDB实例，现在是7005的slave

CLUSTER NODES命令的输出可能看起来有些吓人，但实际上它非常简单，由以下标记组成:

* 节点ID
* ip:port
* flags: master, slave, myself, fail, ... 
* 如果是slave，master的节点ID 
* 上次挂起PING的时间仍在等待答复。
* 上次收到ping和pong的时间。
* 此节点的配置epoch（请参阅群集规范）
* 指向此节点的链接的状态。
* 提供slot槽

## 手动 故障转移

有时，强制故障转移而不实际导致主服务器上出现任何问题是很有用的。
例如，为了升级其中一个主节点的KeyDB进程，最好对其进行故障转移，以便将其转换为对可用性影响最小的从节点。

KeyDB Cluster使用Cluster FAILOVER命令支持手动故障转移，该命令必须在要故障转移的主服务器的一个从服务器中执行。

手动故障切换是特殊的，与实际主故障导致的故障切换相比更安全，因为它们的发生方式避免了过程中的数据丢失，
只在系统确定新主设备处理了旧主设备的所有复制流时，才将客户端从原始主设备切换到新主设备。

这是执行手动故障转移时在从属日志中看到的情况：

    # Manual failover user request accepted.
    # Received replication offset for paused master manual failover: 347540
    # All master replication stream processed, manual failover can start.
    # Start of election delayed for 0 milliseconds (rank #0, offset 347540).
    # Starting a failover election for epoch 7545.
    # Failover election won: I'm the new master.

基本上，连接到我们正在进行故障转移的主服务器的客户机将被停止。
同时，主服务器将其复制偏移发送到从服务器，后者等待到达其一侧的偏移。当达到复制偏移量时，将启动故障转移，并通知旧主机有关配置开关的信息。
当客户端在旧主机上解除阻止时，它们将重定向到新主机。

## 添加新节点

添加一个新节点基本上是添加一个空节点，然后将一些数据移动到其中的过程，如果它是一个新的主节点，或者告诉它设置为一个已知节点的副本，如果它是一个从节点。

我们将展示这两个，首先添加一个新的主实例。

在这两种情况下，要执行的第一步是添加空节点。

这就像在端口7006中启动一个新的节点（我们已经使用了7000到7005，对于我们现有的6个节点），除了端口号之外，使用了与其他节点相同的配置，
所以您应该做些什么来符合我们以前的节点使用的设置：

* 在终端应用程序中创建新选项卡。
* 输入群集测试目录。
* 创建一个名为7006的目录。
* 在内部创建keydb.conf文件，类似于其他节点使用的keydb.conf文件，但使用7006作为端口号。
* 最后用../keydb-server ./keydb.conf启动服务器

此时服务器应该正在运行。
现在我们可以像往常一样使用keydb-cli，以便将节点添加到现有的集群中。

    keydb-cli --cluster add-node 127.0.0.1:7006 127.0.0.1:7000

正如您所看到的，我使用了指定新节点的地址作为第一个参数的add-node命令，以及在集群中随机存在的节点的地址作为第二个参数。

实际上，这里的keydb-cli对我们帮助甚微，它只是向节点发送了一条CLUSTER MEET消息，这也是可以手动完成的。
不过，keydb-cli还会在运行之前检查集群的状态，因此，即使知道内部工作原理，也最好始终通过keydb-cli执行集群操作。

现在我们可以连接到新节点以查看它是否真的加入了群集：

    KeyDB 127.0.0.1:7006> cluster nodes
    3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385543178575 0 connected 5960-10921
    3fc783611028b1707fd65345e763befb36454d73 127.0.0.1:7004 slave 3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 0 1385543179583 0 connected
    f093c80dde814da99c5cf72a7dd01590792b783b :0 myself,master - 0 0 0 connected
    2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543178072 3 connected
    a211e242fc6b22a9427fed61285e85892fa04e08 127.0.0.1:7003 slave 97a3a64667477371c4479320d683e4c8db5858b1 0 1385543178575 0 connected
    97a3a64667477371c4479320d683e4c8db5858b1 127.0.0.1:7000 master - 0 1385543179080 0 connected 0-5959 10922-11422
    3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7005 master - 0 1385543177568 3 connected 11423-16383

注意，由于这个节点已经连接到集群，它已经能够正确地重定向客户机查询，并且通常是集群的一部分。但与其他master相比，它有两个特点：

* 它没有数据，因为它没有分配的哈希槽。
* 因为它是一个没有分配插槽的主机，所以当一个slave想要成为主机时，它不参与选举过程。

现在可以使用keydb-cli的resharding特性将散列槽分配给该节点。
这基本上是没有用的，正如我们已经在前面的一节中所做的，没有区别，它只是一个以空节点为目标的重新硬盘。

## 添加一个新节点作为副本

添加新Replica副本可以通过两种方式执行。最明显的一种方法是再次使用keydb-cli，但是使用--cluster-slave选项，如下所示：

    keydb-cli --cluster add-node 127.0.0.1:7006 127.0.0.1:7000 --cluster-slave

请注意，这里的命令行与我们用来添加新主控形状的命令行完全相同，因此我们没有指定要将副本添加到哪个主控形状。
在这种情况下，keydb-cli将在副本较少的主节点中添加新节点作为随机主节点的副本。

但是，可以使用以下命令行指定新副本的目标主机：

    keydb-cli --cluster add-node 127.0.0.1:7006 127.0.0.1:7000 --cluster-slave --cluster-master-id 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e

这样，我们将新副本分配给一个特定的主副本。

将副本添加到特定主节点的更手动的方法是将新节点添加为空主节点，然后使用CLUSTER REPLICATE命令将其转换为副本。
如果节点被添加为slave节点，但您希望将其作为另一个master节点的副本移动，则此操作也有效。

例如，为了为节点127.0.0.1:7005添加一个副本，该节点当前服务于11423-16383范围内的散列槽，
该节点的节点ID为3c30c74aae0b56170ccb03a76b60cfe7dc1912e，我需要做的只是连接到新节点（已作为空主节点添加）并发送命令：

    KeyDB 127.0.0.1:7006> cluster replicate 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e

就这样。现在我们为这组散列槽创建了一个新副本，集群中的所有其他节点都已经知道了（几秒钟后需要更新它们的配置）。
我们可以使用以下命令进行验证：

    $ keydb-cli -p 7000 cluster nodes | grep slave | grep 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e
    f093c80dde814da99c5cf72a7dd01590792b783b 127.0.0.1:7006 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543617702 3 connected
    2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543617198 3 connected

节点3c3a0c。。。现在有两个奴隶在港口7002（现有的）和7006（新的）上运行。

## 删除一个节点

要删除从属节点，只需使用keydb-cli的del-node命令：

    keydb-cli --cluster del-node 127.0.0.1:7000 `<node-id>`

第一个参数只是集群中的一个随机节点，第二个参数是要删除的节点的ID。

您也可以用同样的方法删除master节点，但是要删除master节点，它必须是空的。
如果master节点不为空，则需要将数据从master节点重新硬存到所有其他master节点。

删除主节点的另一种方法是在其一个slave节点上对其执行手动故障转移，并在该节点变为新master节点的slave节点后将其删除。
显然当您想要减少集群中master服务器的实际数量时，这并没有帮助，在这种情况下，需要resharding。

## 副本迁移

在KeyDB集群中，只要使用以下命令，就可以随时将slave重新配置为使用不同的主机进行复制：

    CLUSTER REPLICATE <master-node-id>

但是，有一种特殊的情况是，您希望副本在没有系统管理员帮助的情况下自动从一个master服务器移动到另一个master服务器。
副本的自动重新配置称为副本迁移，能够提高KeyDB集群的可靠性。

注意：您可以在KeyDB集群规范中阅读副本迁移的详细信息，这里我们只提供一些关于一般思想和您应该做什么以从中受益的信息。

在某些情况下，您可能希望让集群副本从一个master服务器移动到另一个master服务器，
原因是KeyDB集群通常与连接到给定主服务器的副本数量一样，能够抵抗故障。

例如，如果每个主服务器都有一个副本的群集在主服务器及其副本同时失败时无法继续操作，原因很简单，因为没有其他实例具有主服务器正在服务的哈希槽的副本。
然而，虽然netsplits可能会同时隔离多个节点，但许多其他类型的故障，如单个节点本地的硬件或软件故障，是一类非常显著的故障，不太可能同时发生，
因此在每个master节点都有一个slave节点的集群中，slave在凌晨4点被杀，master在早上6点被杀。这仍然会导致集群无法再运行。

为了提高系统的可靠性，我们可以选择向每个master服务器添加额外的副本，但这很昂贵。复制副本迁移允许只向几个master服务器添加更多的slave属服务器。
所以你有10个master和1个slave，总共20次。但是，例如，您添加了3个实例作为某些master服务器的slave服务器，因此某些master服务器将拥有多个master服务器。

对于副本迁移，如果一个master节点没有slave节点，那么一个master节点有多个slave节点的副本将迁移到孤立的主节点。
所以当你的slave像我们上面的例子一样在凌晨4点倒下后，另一个slave将取代它，当主人在早上5点同样失败时，
仍然有一个slave可以被选中，这样集群可以继续运行。

那么，简言之，您应该对复制副本迁移了解多少？

* 群集将尝试从在给定时间内拥有最多副本的主机迁移副本。
* 要从副本迁移中获益，您只需向集群中的单个master节点添加更多副本，而无需考虑哪个master节点。
* 有一个配置参数控制副本迁移功能，称为cluster-migration-barrier:您可以在keydb cluster提供的示例keydb.conf文件中了解更多有关该功能的信息。

## 升级KeyDB集群中的节点

升级slave节点很容易，因为您只需要停止该节点并使用KeyDB的更新版本重新启动它。
如果有客户机使用slave节点扩展读取，那么如果给定的slave节点不可用，它们应该能够重新连接到其他slave节点。

升级master要复杂一些，建议的过程是：

1.使用CLUSTER FAILOVER触发master服务器到其一个slave服务器的手动故障转移（请参阅本文档的"手动故障转移"部分）。
2.等master变成slave。
3.最后升级节点，就像升级slave节点一样。
4.如果希望master节点是刚升级的节点，请触发新的手动故障转移，以便将已升级的节点还原为master节点。

按照此过程，您应该一个接一个地升级一个节点，直到升级所有节点。

## 迁移到KeyDB集群

愿意迁移到KeyDB集群的用户可能只有一个主机，或者可能已经使用了预先存在的共享设置，
其中key在N个节点之间被分割，使用一些内部算法或由其客户端库或KeyDB代理实现的一个分割算法。

在这两种情况下都可以很容易地迁移到KeyDB集群，但是最重要的细节是应用程序是否使用了多个key操作，以及如何使用。
有三种不同的情况：

    1.不使用多个键操作、事务或涉及多个键的Lua脚本。键是独立访问的（即使是通过事务或Lua脚本访问的，这些脚本将多个命令组合在一起，大约是同一个键）。
    
    2.涉及多个键的多个键操作、事务或Lua脚本被使用，但仅与具有相同散列标记的键一起使用，这意味着一起使用的键都具有碰巧相同的{…}子字符串。
    例如，在同一哈希标记的上下文中定义了以下多个键操作：SUNION{user:1000}.foo{user:1000}.bar。
    
    3.涉及多个键的多键操作、事务或Lua脚本与没有显式或相同哈希标记的键名称一起使用。

第三种情况不由KeyDB Cluster处理：需要修改应用程序，以便不使用多个键操作，或者只在同一哈希标记的上下文中使用它们。

案例1和案例2已经介绍过了，所以我们将重点讨论这两个案例，它们的处理方式是相同的，所以在文档中不会做任何区分。

假设您预先存在的数据集被分割成n个主控器，其中n＝1，如果没有预先存在的共享，则需要以下步骤将数据集迁移到KeyDB Cluster：

    1.停止你的clients。当前无法自动实时迁移到KeyDB集群。您可能能够在应用程序/环境的上下文中协调实时迁移。
    
    2.使用BGREWRITEAOF命令为所有N个master生成一个只追加的文件，并等待AOF文件完全生成。
    
    3.将AOF文件从AOF-1保存到AOF-N的某个位置。此时，如果愿意，可以停止旧实例（这很有用，因为在非虚拟化部署中，您经常需要重用相同的计算机）。
    
    4.创建一个由N个master节点和零个slave节点组成的KeyDB集群。稍后您将添加奴隶。确保所有节点都使用append only文件进行持久化。
    
    5.停止所有的群集节点，将它们只附加的文件替换为预先存在的只附加文件，第一节点的AOF-1，第二节点的AOF-2，直到AOF N。
    
    6.使用新的AOF文件重新启动KeyDB集群节点。他们会抱怨，根据他们的配置，有些键不应该在那里。
    
    7.使用keydb-cli --cluster fix命令修复集群，以便根据每个节点是否具有权威性的哈希槽迁移密钥。
    
    8.最后使用keydb-cli --cluster check确保集群正常。
    
    9.重新启动修改为使用KeyDB群集感知客户端库的客户端。

有另一种方法可以将数据从外部实例导入KeyDB集群，即使用 keydb-cli --cluster import命令。

该命令将运行实例的所有键（从源实例中删除键）移动到指定的预先存在的key数据库集群。
但是请注意，如果使用KeyDB 2.8实例作为源实例，则操作可能会很慢，因为2.8没有实现迁移连接缓存，因此您可能希望在执行此操作之前使用KeyDB 3.x版本重新启动源实例。

